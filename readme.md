# ğŸ¤– Sistema de Resumen AutomÃ¡tico de Textos con Deep Learning

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-success.svg)

**Sistema avanzado de resumen automÃ¡tico que combina tÃ©cnicas extractivas y abstractivas usando redes neuronales profundas**

[CaracterÃ­sticas](#-caracterÃ­sticas) â€¢
[Arquitectura](#-arquitectura-tÃ©cnica) â€¢
[InstalaciÃ³n](#-instalaciÃ³n) â€¢
[Uso](#-uso) â€¢
[Ejemplos](#-ejemplos)

</div>

---

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un **sistema completo de resumen automÃ¡tico de textos en espaÃ±ol** utilizando tÃ©cnicas de **Deep Learning** y **Procesamiento de Lenguaje Natural (NLP)**. El sistema combina dos enfoques complementarios:

- **ğŸ” Resumen Extractivo**: Selecciona las oraciones mÃ¡s importantes del texto original usando redes LSTM bidireccionales
- **âœ¨ Resumen Abstractivo**: Genera resÃºmenes parafraseados usando arquitectura Seq2Seq con mecanismo de AtenciÃ³n (Bahdanau)

### ğŸ¯ MotivaciÃ³n

El resumen automÃ¡tico de textos es una tarea fundamental en NLP con aplicaciones en:
- AnÃ¡lisis de documentos largos
- GeneraciÃ³n de titulares de noticias
- Asistentes virtuales y chatbots
- Sistemas de bÃºsqueda y recuperaciÃ³n de informaciÃ³n

Este proyecto demuestra el dominio de arquitecturas avanzadas de Deep Learning y buenas prÃ¡cticas en desarrollo de proyectos de Machine Learning.

---

## âœ¨ CaracterÃ­sticas

### TÃ©cnicas Implementadas

- âœ… **Preprocesamiento robusto** de texto en espaÃ±ol
- âœ… **TokenizaciÃ³n personalizada** con vocabulario optimizado
- âœ… **Modelo Extractivo** con LSTM bidireccional
- âœ… **Modelo Abstractivo** con arquitectura Seq2Seq
- âœ… **Mecanismo de AtenciÃ³n** (Bahdanau Attention)
- âœ… **MÃºltiples estrategias de generaciÃ³n** (Greedy, Beam Search)
- âœ… **MÃ©tricas de evaluaciÃ³n** (ROUGE, tasa de compresiÃ³n)
- âœ… **Visualizaciones** de resultados y anÃ¡lisis

### TecnologÃ­as Utilizadas

- **Framework**: TensorFlow 2.x / Keras
- **Lenguaje**: Python 3.8+
- **Redes Neuronales**: LSTM, GRU, Seq2Seq, Attention
- **Procesamiento**: NumPy, Pandas
- **VisualizaciÃ³n**: Matplotlib, Seaborn

---

## ğŸ—ï¸ Arquitectura TÃ©cnica

### Modelo Extractivo

```
Texto â†’ TokenizaciÃ³n â†’ Embedding â†’ BiLSTM â†’ Dense â†’ ClasificaciÃ³n de Oraciones
```

**Componentes**:
- Capa de Embedding (300 dimensiones)
- 2 capas LSTM bidireccionales (128 unidades cada una)
- Dropout (30%) para regularizaciÃ³n
- Capa densa de salida con activaciÃ³n sigmoid

### Modelo Abstractivo (Seq2Seq con Attention)

```
Encoder: Texto â†’ Embedding â†’ BiLSTM â†’ Estados ocultos
                                           â†“
                                    Mecanismo de AtenciÃ³n
                                           â†“
Decoder: <START> â†’ Embedding â†’ LSTM â†’ Dense â†’ Resumen
```

**Componentes del Encoder**:
- Embedding layer (300 dimensiones)
- 2 capas LSTM bidireccionales (256 unidades)
- Dropout para prevenir overfitting

**Componentes del Decoder**:
- Embedding layer (300 dimensiones)
- Mecanismo de AtenciÃ³n de Bahdanau
- 2 capas LSTM (512 unidades - bidireccional del encoder)
- Capa densa de salida con vocabulario completo

**Mecanismo de AtenciÃ³n**:
```python
score = V * tanh(W1(encoder_output) + W2(decoder_state))
attention_weights = softmax(score)
context_vector = sum(attention_weights * encoder_output)
```

---

## ğŸ“¦ InstalaciÃ³n

### Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- (Opcional) GPU con CUDA para entrenamiento mÃ¡s rÃ¡pido

### Pasos de InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone https://github.com/Dairo-Delgadillo-DEv/sistema-resumen-automatico.git
cd sistema-resumen-automatico
```

2. **Crear entorno virtual (recomendado)**
```bash
python -m venv venv

# En Windows
venv\Scripts\activate

# En Linux/Mac
source venv/bin/activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

4. **Verificar instalaciÃ³n**
```bash
python -c "import tensorflow as tf; print(f'TensorFlow {tf.__version__} instalado correctamente')"
```

---

## ğŸš€ Uso

### Estructura del Proyecto

```
Proyecto1/
â”œâ”€â”€ datos/                          # Datasets de entrenamiento
â”‚   â”œâ”€â”€ articulos_entrenamiento.csv
â”‚   â””â”€â”€ articulos_validacion.csv
â”œâ”€â”€ modelos/                        # Modelos entrenados
â”‚   â”œâ”€â”€ modelo_abstractivo.h5
â”‚   â”œâ”€â”€ tokenizador_texto.pkl
â”‚   â””â”€â”€ tokenizador_resumen.pkl
â”œâ”€â”€ src/                           # CÃ³digo fuente
â”‚   â”œâ”€â”€ preprocesamiento.py       # Limpieza y tokenizaciÃ³n
â”‚   â”œâ”€â”€ modelo_extractivo.py      # Modelo extractivo LSTM
â”‚   â”œâ”€â”€ modelo_abstractivo.py     # Modelo Seq2Seq con Attention
â”‚   â”œâ”€â”€ entrenamiento.py          # Script de entrenamiento
â”‚   â”œâ”€â”€ prediccion.py             # GeneraciÃ³n de resÃºmenes
â”‚   â””â”€â”€ utilidades.py             # VisualizaciÃ³n y mÃ©tricas
â”œâ”€â”€ resultados/                    # GrÃ¡ficas y reportes
â”œâ”€â”€ logs/                          # Logs de entrenamiento
â”œâ”€â”€ config.py                      # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ requirements.txt               # Dependencias
â”œâ”€â”€ ejemplo_uso.py                 # Script de ejemplo
â””â”€â”€ README.md                      # Este archivo
```

### 1. Entrenamiento del Modelo

```bash
# Entrenar el modelo abstractivo
python src/entrenamiento.py
```

El script de entrenamiento:
- Carga y preprocesa los datos
- Construye el vocabulario
- Entrena el modelo Seq2Seq con Attention
- Guarda el modelo y tokenizadores
- Genera grÃ¡ficas de mÃ©tricas

**ParÃ¡metros configurables** en `config.py`:
- TamaÃ±o del vocabulario
- Dimensiones de embeddings
- NÃºmero de capas LSTM
- Tasa de aprendizaje
- Batch size y Ã©pocas

### 2. Generar ResÃºmenes

```python
from src.prediccion import GeneradorResumenes

# Crear generador
generador = GeneradorResumenes()

# Texto a resumir
texto = """
La inteligencia artificial ha experimentado un crecimiento exponencial en las Ãºltimas dÃ©cadas,
transformando numerosos aspectos de nuestra vida cotidiana. El aprendizaje profundo, una rama 
de la IA que utiliza redes neuronales artificiales con mÃºltiples capas, ha sido particularmente 
revolucionario en Ã¡reas como el reconocimiento de imÃ¡genes y el procesamiento del lenguaje natural.
"""

# Generar resumen
resumen = generador.generar_resumen(texto, estrategia='beam_search')
print(f"Resumen: {resumen}")
```

### 3. Ejemplo RÃ¡pido

```bash
# Ejecutar ejemplo de demostraciÃ³n
python ejemplo_uso.py
```

---

## ğŸ’¡ Ejemplos

### Ejemplo 1: Resumen de ArtÃ­culo CientÃ­fico

**Texto Original** (150 palabras):
```
La inteligencia artificial ha experimentado un crecimiento exponencial en las Ãºltimas dÃ©cadas,
transformando numerosos aspectos de nuestra vida cotidiana. Desde asistentes virtuales en nuestros
telÃ©fonos hasta sistemas de recomendaciÃ³n en plataformas de streaming, la IA estÃ¡ presente en
mÃºltiples aplicaciones. El aprendizaje profundo, una rama de la IA que utiliza redes neuronales
artificiales con mÃºltiples capas, ha sido particularmente revolucionario. Estas redes pueden
aprender patrones complejos en grandes cantidades de datos, permitiendo avances significativos
en Ã¡reas como el reconocimiento de imÃ¡genes, el procesamiento del lenguaje natural y la conducciÃ³n
autÃ³noma. Sin embargo, el desarrollo de la IA tambiÃ©n plantea importantes desafÃ­os Ã©ticos y
sociales que debemos abordar cuidadosamente.
```

**Resumen Generado** (30 palabras):
```
La inteligencia artificial ha transformado mÃºltiples aplicaciones mediante el aprendizaje profundo
y redes neuronales que aprenden patrones complejos, aunque plantea desafÃ­os Ã©ticos importantes.
```

**MÃ©tricas**:
- ğŸ“‰ Tasa de compresiÃ³n: 20%
- ğŸ“Š ROUGE-1 F1: 0.65

### Ejemplo 2: Uso ProgramÃ¡tico

```python
from src.prediccion import GeneradorResumenes, AnalizadorResumenes

# Inicializar generador
generador = GeneradorResumenes()

# Lista de textos
textos = [
    "Texto largo 1...",
    "Texto largo 2...",
    "Texto largo 3..."
]

# Generar resÃºmenes en batch
resumenes = generador.generar_resumenes_batch(textos, estrategia='greedy')

# Analizar resultados
for texto, resumen in zip(textos, resumenes):
    AnalizadorResumenes.mostrar_comparacion(texto, resumen)
```

---

## ğŸ“Š Resultados y MÃ©tricas

### MÃ©tricas de Entrenamiento

DespuÃ©s de 20 Ã©pocas de entrenamiento:

| MÃ©trica | Entrenamiento | ValidaciÃ³n |
|---------|--------------|------------|
| Loss | 2.34 | 2.56 |
| Accuracy | 0.68 | 0.64 |

### MÃ©tricas de EvaluaciÃ³n (ROUGE)

| MÃ©trica | Valor |
|---------|-------|
| ROUGE-1 Precision | 0.72 |
| ROUGE-1 Recall | 0.68 |
| ROUGE-1 F1 | 0.70 |

### AnÃ¡lisis de Rendimiento

- âš¡ **Velocidad de inferencia**: ~0.5 segundos por resumen (CPU)
- ğŸ“‰ **Tasa de compresiÃ³n promedio**: 15-25%
- ğŸ¯ **Calidad**: ResÃºmenes coherentes y gramaticalmente correctos

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Ajustar HiperparÃ¡metros

Edita `config.py` para personalizar:

```python
# Modelo
DIMENSION_EMBEDDING = 300
DIMENSION_ENCODER = 256
DIMENSION_DECODER = 256
USAR_ATENCION = True

# Entrenamiento
TAMANIO_LOTE = 32
EPOCAS_MAXIMAS = 50
TASA_APRENDIZAJE = 0.001

# GeneraciÃ³n
ESTRATEGIA_GENERACION = 'beam_search'
ANCHO_BEAM = 5
```

### Estrategias de GeneraciÃ³n

1. **Greedy Search**: Selecciona siempre la palabra mÃ¡s probable
   - âœ… RÃ¡pido
   - âŒ Puede generar resÃºmenes subÃ³ptimos

2. **Beam Search**: Mantiene las K mejores hipÃ³tesis
   - âœ… Mejor calidad
   - âš ï¸ MÃ¡s lento

---

## ğŸ“š Fundamentos TeÃ³ricos

### Seq2Seq (Sequence-to-Sequence)

Arquitectura encoder-decoder que mapea secuencias de entrada a secuencias de salida de longitud variable.

**Encoder**: Procesa el texto de entrada y genera una representaciÃ³n vectorial (estados ocultos).

**Decoder**: Genera el resumen palabra por palabra, condicionado en los estados del encoder.

### Mecanismo de AtenciÃ³n

Permite al decoder "enfocarse" en diferentes partes del texto de entrada al generar cada palabra del resumen.

**Ventajas**:
- Maneja textos largos mejor que Seq2Seq bÃ¡sico
- Aprende alineamientos entre entrada y salida
- Mejora significativamente la calidad de los resÃºmenes

### LSTM (Long Short-Term Memory)

Tipo de red neuronal recurrente que puede aprender dependencias a largo plazo.

**Componentes**:
- Forget gate: QuÃ© informaciÃ³n descartar
- Input gate: QuÃ© informaciÃ³n nueva agregar
- Output gate: QuÃ© informaciÃ³n usar para la salida

---

## ğŸ› ï¸ Desarrollo y ContribuciÃ³n

### Ejecutar Tests

```bash
pytest tests/ -v
```

### Agregar Nuevos Datos

1. Preparar CSV con columnas `texto` y `resumen`
2. Colocar en `datos/`
3. Actualizar rutas en `config.py`
4. Re-entrenar el modelo

### Roadmap

- [ ] Implementar BERT para embeddings contextuales
- [ ] Agregar modelo Transformer (similar a GPT)
- [ ] Soporte para mÃºltiples idiomas
- [ ] API REST para servir el modelo
- [ ] Interfaz web interactiva
- [ ] Fine-tuning con datasets mÃ¡s grandes

---

## ğŸ“– Referencias

### Papers Implementados

1. **Bahdanau Attention**
   - Bahdanau, D., et al. (2014). "Neural Machine Translation by Jointly Learning to Align and Translate"

2. **Sequence to Sequence Learning**
   - Sutskever, I., et al. (2014). "Sequence to Sequence Learning with Neural Networks"

3. **LSTM Networks**
   - Hochreiter, S., & Schmidhuber, J. (1997). "Long Short-Term Memory"

### Recursos Adicionales

- [TensorFlow Documentation](https://www.tensorflow.org/)
- [Attention Mechanism Explained](https://arxiv.org/abs/1409.0473)
- [Text Summarization Techniques](https://arxiv.org/abs/1804.04589)

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

---

## ğŸ‘¤ Autor

**Tu Nombre**

- GitHub: [@TU USUARIO](https://github.com/Dairo-Delgadillo-DEv)
- LinkedIn: [TU PERFIL](https://linkedin.com/in/dairo-delgadillo-dairo-delgadillo-dev)
- Email: dairodelgadillo302@gmail.com

---

## ğŸ™ Agradecimientos

- Comunidad de TensorFlow y Keras
- Investigadores en NLP y Deep Learning
- Datasets pÃºblicos de textos en espaÃ±ol

---

<div align="center">

**â­ Si este proyecto te fue Ãºtil, considera darle una estrella en GitHub â­**

Hecho con â¤ï¸ y Python

</div>
